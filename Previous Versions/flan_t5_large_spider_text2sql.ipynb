{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f25844b505e4273875763922e650d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a20dafedff1460ba5693783c54f9043",
              "IPY_MODEL_ac7a5ca1f38a44fabc0e977b792bb3d7",
              "IPY_MODEL_190a461c896e4545bd436172bafb4f0c"
            ],
            "layout": "IPY_MODEL_ccf57573dc054659a5c774ae65e993a6"
          }
        },
        "9a20dafedff1460ba5693783c54f9043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f82aa20986ce470c9379a15cc9754812",
            "placeholder": "​",
            "style": "IPY_MODEL_d3c2d2d2565044c5a8f8fcb7b714327b",
            "value": "Loading weights: 100%"
          }
        },
        "ac7a5ca1f38a44fabc0e977b792bb3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded8f38e34574c77ba7d9f9965c2496e",
            "max": 558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55e0d3545a2d41f7ba41f4a1e0ae33df",
            "value": 558
          }
        },
        "190a461c896e4545bd436172bafb4f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b672bc52f61743adb301429e3bcd7912",
            "placeholder": "​",
            "style": "IPY_MODEL_3b83263a2d644a6d86a764f1a7e05ff2",
            "value": " 558/558 [00:00&lt;00:00, 1089.46it/s, Materializing param=shared.weight]"
          }
        },
        "ccf57573dc054659a5c774ae65e993a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f82aa20986ce470c9379a15cc9754812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3c2d2d2565044c5a8f8fcb7b714327b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ded8f38e34574c77ba7d9f9965c2496e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e0d3545a2d41f7ba41f4a1e0ae33df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b672bc52f61743adb301429e3bcd7912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b83263a2d644a6d86a764f1a7e05ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# flan-t5-large-spider-text2sql"
      ],
      "metadata": {
        "id": "_wl-KgfZ3IIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "3Qudo9Xl4NTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "MODEL_NAME = 'google/flan-t5-large'\n",
        "SPIDER_DB = 'xlangai/spider'\n",
        "SCHEMA_DB = 'richardr1126/spider-schema'\n",
        "\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "dataset = datasets.load_dataset(SPIDER_DB)\n",
        "schemas = datasets.load_dataset(SCHEMA_DB)\n",
        "\n",
        "# creating schema_dic\n",
        "schema_dic = {}\n",
        "for index in range(0, len(schemas['train'])):\n",
        "    item = schemas['train'][index]\n",
        "    db_id = item['db_id']\n",
        "    schema = item['Schema (values (type))']\n",
        "    schema_dic[db_id] = schema\n",
        "\n",
        "\n",
        "dataset = dataset.remove_columns(\n",
        "    ['query_toks', 'query_toks_no_value', 'question_toks']\n",
        ")\n"
      ],
      "metadata": {
        "id": "wr9yt6IV4MB6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding schemas to the main dataset\n",
        "\n",
        "import re\n",
        "\n",
        "def get_cleaned_schema(schema_raw: str):\n",
        "    \"\"\"\n",
        "    Transforms raw schema string:\n",
        "    \"table : col1 (type) , col2 (type) | table2 : col3 (type)\"\n",
        "\n",
        "    Into clean format:\n",
        "    \"table(col1, col2) | table2(col3)\"\n",
        "    \"\"\"\n",
        "    if not schema_raw:\n",
        "        return \"\"\n",
        "\n",
        "    # Split into separate tables by \"|\"\n",
        "    raw_tables = schema_raw.split('|')\n",
        "    cleaned_parts = []\n",
        "\n",
        "    for raw_table in raw_tables:\n",
        "        # Split Table Name from Columns\n",
        "        parts = raw_table.split(':')\n",
        "\n",
        "        if len(parts) != 2:\n",
        "            continue # Skip malformed entries\n",
        "\n",
        "        table_name = parts[0].strip()\n",
        "        raw_columns = parts[1].strip()\n",
        "\n",
        "        # Clean Columns (Remove types like \"(text)\", \"(number)\")\n",
        "        clean_columns = re.sub(r'\\s*\\([^)]*\\)', '', raw_columns)\n",
        "\n",
        "        # Format as \"table_name(col1, col2)\"\n",
        "        cols_list = [c.strip() for c in clean_columns.split(',')]\n",
        "        formatted_cols = \", \".join(cols_list)\n",
        "\n",
        "        cleaned_parts.append(f\"{table_name}({formatted_cols})\")\n",
        "\n",
        "    return \" | \".join(cleaned_parts)\n",
        "\n",
        "\n",
        "\n",
        "def fix_question(question, db_id, prefix):\n",
        "    '''\n",
        "        fixes a single question\n",
        "        adds prefix to the beginning\n",
        "        adds the schema at the end\n",
        "        the output format looks like this:\n",
        "            <prefix>: <question> | Schemas: <schema-1>(column-1, column-2, ...) | <schema-2>(column-1, column-2, ...)\n",
        "    '''\n",
        "    schema = schema_dic[db_id]\n",
        "    cleanted_schema = get_cleaned_schema(schema)\n",
        "    output = prefix + \": \" + question + \" | \" + \"Schemas: \" + cleanted_schema\n",
        "    return output\n",
        "\n",
        "\n",
        "def preprocess(batch):\n",
        "    task_prefix = 'Translate English to SQL'\n",
        "    query = batch['query']\n",
        "\n",
        "    fixed_questions_list = []\n",
        "    for sample_db_id, sample_question in zip(batch['db_id'], batch['question']):\n",
        "        fixed_questions_list.append(fix_question(sample_question, sample_db_id, task_prefix))\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        fixed_questions_list,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        text_target=query,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "JIHWSb1k46mw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# apply preprocessing\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    model_max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "dataset['train'] = dataset['train'].map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    batch_size=64,\n",
        "    remove_columns=dataset['train'].column_names,\n",
        "    num_proc=(os.cpu_count() // 3) + 1\n",
        ")\n",
        "\n",
        "dataset['validation'] = dataset['validation'].map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    batch_size=64,\n",
        "    remove_columns=dataset['validation'].column_names,\n",
        "    num_proc=(os.cpu_count() // 3) + 1\n",
        ")\n",
        "\n",
        "# split validation set to test and validation\n",
        "val_split = dataset['validation'].train_test_split(test_size=0.5, seed=42)\n",
        "dataset['test'] = val_split['train']\n",
        "dataset['validation'] = val_split['test']\n",
        "del val_split\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSBSXJU048Rh",
        "outputId": "276c0f68-eb56-4ba1-b0a2-741bd3b39bd4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 7000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 517\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 517\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup"
      ],
      "metadata": {
        "id": "4shmqeWv9kgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compute_metrics & CallBacks"
      ],
      "metadata": {
        "id": "jxOzpwRLJlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U evaluate -q\n",
        "!pip install -U rouge_score -q\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    def normalize_sql(s):\n",
        "        return s.lower().replace(\" \", \"\").strip()\n",
        "\n",
        "    exact_matches = [\n",
        "        1 if normalize_sql(p) == normalize_sql(l) else 0\n",
        "        for p, l in zip(decoded_preds, decoded_labels)\n",
        "    ]\n",
        "\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": sum(exact_matches) / len(exact_matches),\n",
        "        \"rouge1\": result[\"rouge1\"],\n",
        "    }\n",
        "\n",
        "\n",
        "class PrinterCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        print(\"\\n*** DEBUGGING GENERATION ***\")\n",
        "\n",
        "        # Hardcode a fixed example — no dataset access needed\n",
        "        input_text = \"Translate English to SQL: How many players are from each country? | Schemas: players(player_id, first_name, last_name, hand, birth_date, country_code) | matches(best_of, draw_size, loser_age, loser_entry, loser_hand, loser_ht, loser_id, loser_ioc, loser_name, loser_rank, loser_rank_points, loser_seed, match_num, minutes, round, score, surface, tourney_date, tourney_id, tourney_level, tourney_name, winner_age, winner_entry, winner_hand, winner_ht, winner_id, winner_ioc, winner_name, winner_rank, winner_rank_points, winner_seed, year) | rankings(ranking_date, ranking, player_id, ranking_points, tours\"\n",
        "        gold_label = \"SELECT count(*) , country_code FROM players GROUP BY country_code\"\n",
        "\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            gen_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=GENERATION_MAX_LENGTH,\n",
        "                num_beams=4,\n",
        "                repetition_penalty=1.2,\n",
        "            )\n",
        "\n",
        "        decoded_pred = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "        print(f\"QUESTION: How many players are from each country?\")\n",
        "        print(f\"PREDICTION: {decoded_pred}\")\n",
        "        print(f\"GOLD LABEL: {gold_label}\")\n",
        "        print(\"******************************\\n\")"
      ],
      "metadata": {
        "id": "k_b9twHvJcFm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Args"
      ],
      "metadata": {
        "id": "BEx5QoA0Jmgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Seq2SeqTrainingArguments, GenerationConfig\n",
        "from pathlib import Path\n",
        "\n",
        "SAVE_DIR = Path('./models')\n",
        "TRAIN_BATCH_SIZE = 4 # BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS -> true batch size\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LR = 0.0003\n",
        "MAX_GRAD_NORM = 2\n",
        "EPOCH = 10\n",
        "WEIGHT_DECAY = 0.0005\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "WARMUP_RATIO = 0.1\n",
        "LABEL_SMOOTHING = 0\n",
        "GENERATION_MAX_LENGTH = 128\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=SAVE_DIR,\n",
        "\n",
        "    # --- OPTIMIZER ---\n",
        "    # optim=\"adafactor\",\n",
        "    optim=\"adamw_torch\",\n",
        "    # torch_compile=True,\n",
        "    # torch_compile_backend=\"inductor\",\n",
        "    # torch_compile_mode=\"reduce-overhead\",\n",
        "\n",
        "    # --- EVALUATION STRATEGY ---\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=4,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='exact_match',\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # --- BATCHING & OPTIMIZATION ---\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    num_train_epochs=EPOCH,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    label_smoothing_factor=LABEL_SMOOTHING,\n",
        "    lr_scheduler_type='cosine',\n",
        "\n",
        "    # --- GENERATION ---\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=GENERATION_MAX_LENGTH,\n",
        "\n",
        "    # --- LOGGING & HARDWARE ---\n",
        "    logging_steps=2,            # 1 is too noisy, 10 is cleaner\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    report_to='none',\n",
        "\n",
        "    # -----\n",
        "    generation_config=GenerationConfig(\n",
        "        # repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        decoder_start_token_id=tokenizer.pad_token_id,\n",
        "        num_beams=4,\n",
        "    ),\n",
        "    dataloader_num_workers=(os.cpu_count() //3 * 2),\n",
        "    group_by_length=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4rFP0yv_ARl",
        "outputId": "bf710574-a2b0-41b3-9ed9-77d31c5b94b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Setup"
      ],
      "metadata": {
        "id": "TgGtqBXjJqPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model\n",
        "\n",
        "#!pip install peft\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_NAME,\n",
        "    # torch_dtype=torch.bfloat16  # Load directly in BF16\n",
        ")\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],  # include FFN this time\n",
        "    lora_dropout=0.0,\n",
        "    bias='none',\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "0f25844b505e4273875763922e650d93",
            "9a20dafedff1460ba5693783c54f9043",
            "ac7a5ca1f38a44fabc0e977b792bb3d7",
            "190a461c896e4545bd436172bafb4f0c",
            "ccf57573dc054659a5c774ae65e993a6",
            "f82aa20986ce470c9379a15cc9754812",
            "d3c2d2d2565044c5a8f8fcb7b714327b",
            "ded8f38e34574c77ba7d9f9965c2496e",
            "55e0d3545a2d41f7ba41f4a1e0ae33df",
            "b672bc52f61743adb301429e3bcd7912",
            "3b83263a2d644a6d86a764f1a7e05ff2"
          ]
        },
        "id": "KPl6tmbxGFmG",
        "outputId": "18f87fa4-436d-4317-8f56-01fd2d9e195c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/558 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f25844b505e4273875763922e650d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cast base model to bf16, but keep LoRA adapters in fp32\n",
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         param.data = param.data.to(torch.float32)  # LoRA stays fp32\n",
        "#     else:\n",
        "#         param.data = param.data.to(torch.bfloat16)  # base stays bf16\n",
        "\n",
        "\n",
        "# # Verify\n",
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(f\"TRAINABLE: {param.dtype}\")  # should be float32\n",
        "#         break\n",
        "# for name, param in model.named_parameters():\n",
        "#     if not param.requires_grad:\n",
        "#         print(f\"FROZEN: {param.dtype}\")  # should be bfloat16\n",
        "#         break"
      ],
      "metadata": {
        "id": "wxAxvGmeGIAz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import AdamW\n",
        "\n",
        "# # Recreate batch\n",
        "# loader = trainer.get_train_dataloader()\n",
        "# batch = next(iter(loader))\n",
        "# batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# for step in range(100):\n",
        "#     optimizer.zero_grad()\n",
        "#     outputs = model(**batch)\n",
        "#     loss = outputs.loss\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if step % 10 == 0:\n",
        "#         print(f\"Step {step}: loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "zcWnOyDL_59A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data collator\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator_fn = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    label_pad_token_id=-100, # default is -100 (-100 will be automatically ignored by PyTorch loss functions)\n",
        "    pad_to_multiple_of=8,  # Optimization for TPU/GPU cores\n",
        ")\n",
        "\n",
        "# trainer\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator_fn,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[PrinterCallback],\n",
        ")"
      ],
      "metadata": {
        "id": "gcENJz6z_3At"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "u55nwlPi_6_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "5j27J9Yv_7hN",
        "outputId": "02b0d2f9-4487-4d41-f952-7fa2ee54ea59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='657' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 657/4380 27:57 < 2:38:56, 0.39 it/s, Epoch 1.50/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>Rouge1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>11.822417</td>\n",
              "      <td>3.050445</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.307812</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** DEBUGGING GENERATION ***\n",
            "QUESTION: How many players are from each country?\n",
            "PREDICTION: SELECT count(*) FROM player_id\n",
            "GOLD LABEL: SELECT count(*) , country_code FROM players GROUP BY country_code\n",
            "******************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTx8xXakJ_D3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}